{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206bdc2f",
   "metadata": {},
   "source": [
    "# **Linear Regression - rice_ml**\n",
    "This notebook demonstrates how to use the LinearRegression class within the rice_ml package. It demonstrates it in an informative way that also analyzes the results, mirroring a standard use case of the classes.\n",
    "\n",
    "Note that when using this in robust model selection, k-fold cross-validation and deeper hyperparameter tuning is recommended. In this example, since it's main goal is demonstrating the classes, we will not do as deep of hyperparameter tuning, and will compare every test using the same random state (42).\n",
    "\n",
    "This notebook shows how to:\n",
    "- Use 'LinearRegression' from 'rice_ml'\n",
    "- Prepare and normalize data using 'rice_ml'\n",
    "- Evaluate decision trees on a regression task\n",
    "\n",
    "## Table of Contents\n",
    "- [Algorithm](#algorithm)\n",
    "- [Data Preparation](#data-preparation)\n",
    "- [Linear Regression](#linear-regression)\n",
    "  - [Model Training](#model-training)\n",
    "  - [Results](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5770c7a4",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "Linear regression is a supervised learning algorithm that models the relationship between a set of input features and a continuous target variable by fitting a linear equation to the data. The model assumes that the target can be expressed as a weighted sum of the input features plus a bias term.\n",
    "\n",
    "The algorithm learns the optimal coefficients by minimizing a loss function, typically mean squared error (MSE), which measures the average squared difference between predicted and actual values.\n",
    "\n",
    "During training, linear regression adjusts its weights so that the predicted values lie as close as possible to the observed data points in a least-squares sense. Each feature contributes linearly to the final prediction, scaled by its learned coefficient.\n",
    "\n",
    "To make a prediction, the model computes the dot product of the input features and the learned weights, then adds the intercept term. The resulting value is returned as the predicted output.\n",
    "\n",
    "Linear regression is simple, fast, and highly interpretable, making it a strong baseline model. However, its performance depends heavily on the assumption of linearity and can degrade when relationships between features and targets are nonlinear or when strong outliers are present.\n",
    "\n",
    "![Linear Regression Example](../images/lin_reg.webp)\n",
    "Source: [SSDSI](https://sixsigmadsi.com/glossary/simple-linear-regression/)\n",
    "\n",
    "\n",
    "### Pros vs Cons\n",
    "#### Pros\n",
    "- Very interpretable (coefficients provide great insight into feature importance)\n",
    "- Fast to train and to predict\n",
    "- Works very well with approximately linear relationships\n",
    "#### Cons\n",
    "- Assumes linear relationships (does not work with nonlinear relationships at all)\n",
    "- Sensitive to outliers\n",
    "- Can often require careful feature engineering and transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168214a9",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We will be using the California Housing data from 'sklearn'. It contains census information about median housing prices in California districts. Given demographic and geographic features, we can use it to predict the median house value for a district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7da7091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (20640, 8)\n",
      "y shape: (20640,)\n",
      "Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "\n",
    "data = fetch_california_housing()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"Features:\", feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb0024",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665b467",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba4888c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.610\n",
      "RMSE: 0.725\n"
     ]
    }
   ],
   "source": [
    "from rice_ml.supervised_learning.linear_regression import LinearRegression\n",
    "from rice_ml.utilities.preprocess import train_test_split, normalize\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "r2 = model.score(X_test, y_test)\n",
    "print(f\"R2 score: {r2:.3f}\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = ((y_test - y_pred) ** 2).mean()\n",
    "print(f\"RMSE: {np.sqrt(mse):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f762e12",
   "metadata": {},
   "source": [
    "The base model has a decent R^2 score. It is explaining around 61% of the variance. The RMSE is pretty good. It is in terms of 100,000 USD, so it is showing the average error at around 72,500 USD.\n",
    "Next we will look at some parameter tuning and normalization.\n",
    "\n",
    "Linear regression is simple and very interpretable. It is often used as a base model for comparison. This is a decent result, and is a great baseline to compare to other models for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165036b7",
   "metadata": {},
   "source": [
    "#### Z-Score Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca79650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.610\n",
      "RMSE: 0.725\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Normalize on the training data to avoid data leakage.\n",
    "X_train, stats = normalize(X_train, method=\"zscore\", return_stats=True)\n",
    "X_test = normalize(X_test, method=\"zscore\", stats=stats)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "r2 = model.score(X_test, y_test)\n",
    "print(f\"R2 score: {r2:.3f}\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = ((y_test - y_pred) ** 2).mean()\n",
    "print(f\"RMSE: {np.sqrt(mse):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a91408",
   "metadata": {},
   "source": [
    "### Results\n",
    "This actually does not improve the model at all. While normalization does not have a major effect on linear regression, it does have a different benefit. It is recommended to still normalize your data, as it greatly improves feature interpretability and feature importance clarity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
