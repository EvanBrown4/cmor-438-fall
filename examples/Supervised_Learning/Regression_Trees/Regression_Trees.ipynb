{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206bdc2f",
   "metadata": {},
   "source": [
    "# **Regression Trees - rice_ml**\n",
    "This notebook demonstrates how to use the DecisionTreeRegressor class within the rice_ml package. It demonstrates it in an informative way that also analyzes the results, mirroring a standard use case of the classes.\n",
    "\n",
    "Note that when using this in robust model selection, k-fold cross-validation and deeper hyperparameter tuning is recommended. In this example, since it's main goal is demonstrating the classes, we will not do as deep of hyperparameter tuning, and will compare every test using the same random state (42).\n",
    "\n",
    "This notebook shows how to:\n",
    "- Use 'DecisionTreeRegressor' from 'rice_ml'\n",
    "- Prepare and normalize data using 'rice_ml'\n",
    "- Evaluate decision trees on a regression task\n",
    "\n",
    "## Table of Contents\n",
    "- [Algorithm](#algorithm)\n",
    "- [Data Preparation](#data-preparation)\n",
    "- [Decision Tree Regression](#decision-tree-regression)\n",
    "  - [Model Training](#model-training)\n",
    "  - [Results](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5770c7a4",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "A regression tree is a supervised learning algorithm that models a continuous value by recursively dividing the feature space into regions and fitting a constant prediction within each region. Unlike classification trees, which predict labels, regression trees predict numeric values.\n",
    "\n",
    "The algorithm repeatedly selects a feature and a value to split it at that separates the data the best. It uses mean squared error (MSE) to decide which split is best. At each node in the tree, it chooses the split that results in the lowest MSE, continuing down the tree until a leaf node is reached. Each node represents a decision rule, and every leaf represents a prediction value.\n",
    "\n",
    "To make a prediction, a data point is passed through the tree, moving down towards as leaf as dicated by the decision rules it reaches. Once it reaches a leaf, that predicted value is returned as the data point's prediction.\n",
    "\n",
    "Regression trees are great at modeling nonlinear relationships without needing any feature transformations, but they rely on the quality and structure of the splits chosen during training. They can easily be overfit if the wrong parameters are used.\n",
    "\n",
    "A great way to counter overfitting is using an ensemble method such as a random forest. A regression tree is a great baseline model for a random forest.\n",
    "\n",
    "![Decision Tree Example](../images/regressor_tree.png)\n",
    "Source: [Medium](https://medium.com/@sametgirgin/decision-tree-regression-in-6-steps-with-python-1a1c5aa2ee16)\n",
    "\n",
    "\n",
    "### Pros vs Cons\n",
    "#### Pros\n",
    "- Interpretable and easy to visualize\n",
    "- Handles nonlinear relationships very well\n",
    "- Works with mixed feature types (numerical and categorical can be handled directly)\n",
    "- Fast predictions\n",
    "#### Cons\n",
    "- Can easily overfit\n",
    "- High variance\n",
    "- Constant predictions within each leaf (can cause sharp and unrealistic jumps in output if there are not enough leafs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168214a9",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We will be using the California Housing data from 'sklearn'. It contains census information about median housing prices in California districts. Given demographic and geographic features, we can use it to predict the median house value for a district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7da7091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (20640, 8)\n",
      "y shape: (20640,)\n",
      "Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "\n",
    "data = fetch_california_housing()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"Features:\", feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb0024",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665b467",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ba4888c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.570\n",
      "RMSE: 0.761\n"
     ]
    }
   ],
   "source": [
    "from rice_ml.supervised_learning.regression_trees import DecisionTreeRegressor\n",
    "from rice_ml.utilities.preprocess import train_test_split, normalize\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model = DecisionTreeRegressor(\n",
    "    max_depth=4,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "r2 = model.score(X_test, y_test)\n",
    "print(f\"R2 score: {r2:.3f}\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = ((y_test - y_pred) ** 2).mean()\n",
    "print(f\"RMSE: {np.sqrt(mse):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f762e12",
   "metadata": {},
   "source": [
    "The base model has a decent R^2 score, but could definitely be much better. It is explaining around 57% of the variance. The RMSE is pretty good. It is in terms of $100k, so it is showing the average error at around $76,000.\n",
    "Next we will look at some parameter tuning and normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f60bf",
   "metadata": {},
   "source": [
    "#### Effect of max_depth on Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2943d079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 (max_depth=1): 0.310\n",
      "RMSE (max_depth=1): 0.964\n",
      "R^2 (max_depth=2): 0.447\n",
      "RMSE (max_depth=2): 0.863\n",
      "R^2 (max_depth=3): 0.524\n",
      "RMSE (max_depth=3): 0.800\n",
      "R^2 (max_depth=4): 0.570\n",
      "RMSE (max_depth=4): 0.761\n",
      "R^2 (max_depth=5): 0.619\n",
      "RMSE (max_depth=5): 0.716\n",
      "R^2 (max_depth=8): 0.696\n",
      "RMSE (max_depth=8): 0.639\n",
      "R^2 (max_depth=10): 0.687\n",
      "RMSE (max_depth=10): 0.649\n",
      "R^2 (max_depth=None): 0.615\n",
      "RMSE (max_depth=None): 0.720\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nn = [1, 2, 3, 4, 5, 8, 10, None]\n",
    "\n",
    "for n in nn:\n",
    "    model = DecisionTreeRegressor(\n",
    "        max_depth=n,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    results = model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = ((y_test - y_pred) ** 2).mean()\n",
    "\n",
    "    print(f\"R^2 (max_depth={n}): {score:.3f}\")\n",
    "    print(f\"RMSE (max_depth={n}): {np.sqrt(mse):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c0425",
   "metadata": {},
   "source": [
    "The max depth in a decision tree is crucial. Tuning it based on your data is very important and can change a horrible model to a great model, and vise versa. Tuning min_samples_split, min_samples_leaf, and max_features can also have huge benefits.\n",
    "\n",
    "In this case, we can see that our data performs much better as the depth increases, but tops out around 8 levels. This shows that we need enough layers to make close enough decisions, but too many layers results in overfitting.\n",
    "\n",
    "We will use max_depth=8 for the normalization test. Let's see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165036b7",
   "metadata": {},
   "source": [
    "#### Z-Score Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca79650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.696\n",
      "RMSE: 0.639\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Normalize on the training data to avoid data leakage.\n",
    "X_train, stats = normalize(X_train, method=\"zscore\", return_stats=True)\n",
    "X_test = normalize(X_test, method=\"zscore\", stats=stats)\n",
    "\n",
    "model = DecisionTreeRegressor(\n",
    "    max_depth=8,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "r2 = model.score(X_test, y_test)\n",
    "print(f\"R2 score: {r2:.3f}\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = ((y_test - y_pred) ** 2).mean()\n",
    "print(f\"RMSE: {np.sqrt(mse):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a91408",
   "metadata": {},
   "source": [
    "### Results\n",
    "This actually does not improve the model at all. This is a key piece of trees: normalization is not normally necessary. The trees split based on single thresholds, so the data itself does not need to be normalized. It will act in the same way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
