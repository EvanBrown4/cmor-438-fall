{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f28b401",
   "metadata": {},
   "source": [
    "    This implementation uses ReLU activation for hidden layers and sigmoid\n",
    "    activation for the output layer. Training is performed using backpropagation\n",
    "    with binary cross-entropy loss. The network can have an arbitrary number\n",
    "    of hidden layers with configurable sizes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
