# Supervised Learning Examples

This folder contains example notebooks for **supervised learning algorithms** implemented in the `rice_ml` package.
Each example demonstrates how to train, evaluate, and analyze a model on labeled data, with an emphasis on understanding model behavior, assumptions, and performance trade-offs.

The notebooks are intended to be **educational and exploratory**, not just minimal usage demonstrations.

---

## Algorithms Included

- **Linear Regression**  
  Predicts continuous target values by modeling a linear relationship between features and the response variable.

- **Logistic Regression**  
  A probabilistic linear classifier used for binary classification tasks.

- **K-Nearest Neighbors (KNN)**  
  - **Classification** – Predicts class labels based on the majority vote of nearby points.  
  - **Regression** – Predicts continuous values by averaging the targets of nearby points.

- **Perceptron (Classification)**  
  A linear binary classifier trained using an online learning rule, serving as a foundational neural model.

- **Multilayer Perceptron (MLP) (Classification)**  
  A feedforward neural network with one or more hidden layers, capable of modeling non-linear decision boundaries.

- **Decision Trees (Classification)**  
  Tree-based models that recursively split the feature space to make class predictions.

- **Regression Trees (Regression)**  
  Tree-based models for predicting continuous targets using piecewise constant approximations.

- **Ensemble Methods (Random Forest)**  
  - **Classification** – Aggregates multiple decision trees to improve classification stability and accuracy.  
  - **Regression** – Uses an ensemble of regression trees to reduce variance and improve predictive performance.

---

## When to Use These Algorithms

Supervised learning algorithms are appropriate when:
- You have **labeled data** (input–output pairs)
- The goal is to **predict a known target variable**
- You want to understand relationships between features and outcomes, make predictions, or compare model performance

Different algorithms are suited to different settings:
- **Linear and Logistic Regression** for simple, interpretable models
- **KNN** for non-parametric, instance-based learning
- **Perceptron and MLP** for learning linear and non-linear decision boundaries
- **Decision Trees and Random Forests** for flexible, non-linear models with minimal preprocessing

The examples often compare multiple models on the same dataset to highlight these trade-offs.

---

## How to Navigate

Each algorithm has its own subfolder containing:
- A short **README** describing the algorithm and example setup
- One or more **Jupyter notebooks** demonstrating:
  - Data preprocessing
  - Model training
  - Hyperparameter selection
  - Evaluation metrics
  - Visualizations and error analysis
- Any **supporting plots or figures** generated by the notebooks

Notebooks are designed to be read top-to-bottom and include explanatory markdown alongside code.
